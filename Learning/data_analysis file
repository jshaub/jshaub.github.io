import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer 
from sklearn.feature_extraction.text import TfidfVectorizer 


df = pd.read_csv(PATH + "train.csv", index_col="PersonId")
df.target = df.ClusterClassifier.astype('category')
df.head()
import operator

ITEMS = 15
clusters=["Sports","Computer Nerds","Anime","Computer Science","Math","Theater","Sing and Dance","Music", "Movies", "Food"]
v = CountVectorizer(ngram_range=(1,3),max_features=ITEMS)
for i in range(1,8):
    df_x = v.fit_transform(df[df.Target == i].Interests.values.astype('U'))
    print("\nTop",ITEMS,"Interests for",clusters[i],"People")
    n = 1
    d = v.get_feature_names()
    c = df_x.toarray().sum(axis=0)
    counts = pd.DataFrame({'term': d, 'term_count': c})
    print(counts.sort_values(['term_count'],ascending=0).head(n=ITEMS).to_string(index=False))

ITEMS = 50
v = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3),max_features=ITEMS)
df_x = v.fit_transform(df.Interests.values.astype('U'))
idf = v.idf_

topInterests = pd.DataFrame({"Interest": v.get_feature_names(),"TfIdf":idf})
print("Top",ITEMS,"Interests")
topInterests.sort_values(by="TfIdf",ascending=False)
//
import nltk
from nltk.stem.porter import PorterStemmer
  
def tokenize(text):
    tokens = nltk.word_tokenize(text)
    stems = []
    for item in tokens:
        stems.append(PorterStemmer().stem(item))
    return stems

ITEMS = 50
v = TfidfVectorizer(sublinear_tf=True, tokenizer=tokenize, stop_words='english', ngram_range=(1,3),max_features=ITEMS)
df_x = v.fit_transform(df.Interests.values.astype('U'))
idf = v.idf_
 
topInterests = pd.DataFrame({"Interest": v.get_feature_names(),"TfIdf":idf})
print("Top",ITEMS,"Interests")
topInterests.sort_values(by="TfIdf",ascending=False)




